{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-3251fa9f1525>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3251fa9f1525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsparkSession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    337\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-3251fa9f1525>:3 "
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc, jsparkSession=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chapters\n",
    "bookChaptersDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"bookcontents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+\n",
      "|Chapter|                Name|Page|\n",
      "+-------+--------------------+----+\n",
      "|      1|        Introduction|  11|\n",
      "|      2|Basic Engineering...|  19|\n",
      "|      3|Advanced Engineer...|  28|\n",
      "|      4|     Hands On Course|  60|\n",
      "|      5|        Case Studies|  62|\n",
      "|      6|Best Practices Cl...|  73|\n",
      "|      7|130+ Data Sources...|  77|\n",
      "|      8|1001 Interview Qu...|  82|\n",
      "|      9|Recommended Books...|  87|\n",
      "+-------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookChaptersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Chapter: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Page: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookChaptersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sections\n",
    "sectionsDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"sections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Chapter: integer (nullable = true)\n",
      " |-- Section: double (nullable = true)\n",
      " |-- Section_Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sectionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|Chapter|Section|        Section_Name|\n",
      "+-------+-------+--------------------+\n",
      "|      5|    5.1| Data Science Airbnb|\n",
      "|      5|    5.2| Data Science Amazon|\n",
      "|      5|    5.3|  Data Science Baidu|\n",
      "|      5|    5.4|Data Science Blac...|\n",
      "|      5|    5.5|    Data Science BMW|\n",
      "|      5|    5.6|Data Science Book...|\n",
      "|      5|    5.7|   Data Science CERN|\n",
      "|      5|    5.8| Data Science Disney|\n",
      "|      5|    5.9|    Data Science DLR|\n",
      "|      5|    5.1|Data Science Driv...|\n",
      "|      5|   5.11|Data Science Dropbox|\n",
      "|      5|   5.12|   Data Science Ebay|\n",
      "|      5|   5.13|Data Science Expedia|\n",
      "|      5|   5.14|Data Science Face...|\n",
      "|      5|   5.15| Data Science Google|\n",
      "|      5|   5.16|Data Science Gram...|\n",
      "|      5|   5.17|Data Science ING ...|\n",
      "|      5|   5.18|Data Science Inst...|\n",
      "|      5|   5.19|Data Science Link...|\n",
      "|      5|    5.2|   Data Science Lyft|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sectionsDF.where(\"Chapter == 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+-------+--------------------+\n",
      "|Chapter|                Name|Page|Section|        Section_Name|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "|      1|        Introduction|  11|    1.1|What is this Cook...|\n",
      "|      1|        Introduction|  11|    1.2|Data Engineer vs ...|\n",
      "|      1|        Introduction|  11|    1.3|My Data Science P...|\n",
      "|      1|        Introduction|  11|    1.4|  Who Companies Need|\n",
      "|      2|Basic Engineering...|  19|    2.1|       Learn To Code|\n",
      "|      2|Basic Engineering...|  19|    2.2|Get Familiar With...|\n",
      "|      2|Basic Engineering...|  19|    2.3|   Agile Development|\n",
      "|      2|Basic Engineering...|  19|    2.4|Software Engineer...|\n",
      "|      2|Basic Engineering...|  19|    2.5|Learn how a Compu...|\n",
      "|      2|Basic Engineering...|  19|    2.6|Data Network Tran...|\n",
      "|      2|Basic Engineering...|  19|    2.7|Security and Privacy|\n",
      "|      2|Basic Engineering...|  19|    2.8|               Linux|\n",
      "|      2|Basic Engineering...|  19|    2.9|              Docker|\n",
      "|      2|Basic Engineering...|  19|    2.1|           The Cloud|\n",
      "|      2|Basic Engineering...|  19|   2.11|Security Zone Design|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join options\n",
    "#inner,outer,left_outer,right_outer,leftsemi\n",
    "bookChaptersDF.join(sectionsDF,\"Chapter\").where(\"Chapter == 1 or Chapter == 2 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcontentDF = bookChaptersDF.join(sectionsDF,\"Chapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Chapter'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookcontentDF.Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|        Section_Name|Chapter|\n",
      "+--------------------+-------+\n",
      "|What is this Cook...|      1|\n",
      "|Data Engineer vs ...|      1|\n",
      "|My Data Science P...|      1|\n",
      "|  Who Companies Need|      1|\n",
      "|       Learn To Code|      2|\n",
      "|Get Familiar With...|      2|\n",
      "|   Agile Development|      2|\n",
      "|Software Engineer...|      2|\n",
      "|Learn how a Compu...|      2|\n",
      "|Data Network Tran...|      2|\n",
      "|Security and Privacy|      2|\n",
      "|               Linux|      2|\n",
      "|              Docker|      2|\n",
      "|           The Cloud|      2|\n",
      "|Security Zone Design|      2|\n",
      "|Data Science Plat...|      3|\n",
      "|    Hadoop Platforms|      3|\n",
      "|             Connect|      3|\n",
      "|              Buffer|      3|\n",
      "|Processing Framew...|      3|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookcontentDF.select(bookcontentDF.Section_Name,\"Chapter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Columns Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|        Section_Name|(Chapter * 10)|\n",
      "+--------------------+--------------+\n",
      "|What is this Cook...|            10|\n",
      "|Data Engineer vs ...|            10|\n",
      "|My Data Science P...|            10|\n",
      "|  Who Companies Need|            10|\n",
      "|       Learn To Code|            20|\n",
      "|Get Familiar With...|            20|\n",
      "|   Agile Development|            20|\n",
      "|Software Engineer...|            20|\n",
      "|Learn how a Compu...|            20|\n",
      "|Data Network Tran...|            20|\n",
      "|Security and Privacy|            20|\n",
      "|               Linux|            20|\n",
      "|              Docker|            20|\n",
      "|           The Cloud|            20|\n",
      "|Security Zone Design|            20|\n",
      "|Data Science Plat...|            30|\n",
      "|    Hadoop Platforms|            30|\n",
      "|             Connect|            30|\n",
      "|              Buffer|            30|\n",
      "|Processing Framew...|            30|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiply values\n",
    "bookcontentDF.select(\"Section_Name\", bookcontentDF.Chapter * 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|        Section_Name|Chapter_10|\n",
      "+--------------------+----------+\n",
      "|What is this Cook...|        10|\n",
      "|Data Engineer vs ...|        10|\n",
      "|My Data Science P...|        10|\n",
      "|  Who Companies Need|        10|\n",
      "|       Learn To Code|        20|\n",
      "|Get Familiar With...|        20|\n",
      "|   Agile Development|        20|\n",
      "|Software Engineer...|        20|\n",
      "|Learn how a Compu...|        20|\n",
      "|Data Network Tran...|        20|\n",
      "|Security and Privacy|        20|\n",
      "|               Linux|        20|\n",
      "|              Docker|        20|\n",
      "|           The Cloud|        20|\n",
      "|Security Zone Design|        20|\n",
      "|Data Science Plat...|        30|\n",
      "|    Hadoop Platforms|        30|\n",
      "|             Connect|        30|\n",
      "|              Buffer|        30|\n",
      "|Processing Framew...|        30|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use aliases for columns\n",
    "bookcontentDF.select(\"Section_Name\", (bookcontentDF.Chapter * 10).alias(\"Chapter_10\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+-------+--------------------+\n",
      "|Chapter|                Name|Page|Section|        Section_Name|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "|      1|        Introduction|  11|    1.2|Data Engineer vs ...|\n",
      "|      1|        Introduction|  11|    1.3|My Data Science P...|\n",
      "|      2|Basic Engineering...|  19|    2.6|Data Network Tran...|\n",
      "|      3|Advanced Engineer...|  28|    3.1|Data Science Plat...|\n",
      "|      4|     Hands On Course|  60|    4.6|Apache Zeppelin D...|\n",
      "|      5|        Case Studies|  62|    5.1| Data Science Airbnb|\n",
      "|      5|        Case Studies|  62|    5.2| Data Science Amazon|\n",
      "|      5|        Case Studies|  62|    5.3|  Data Science Baidu|\n",
      "|      5|        Case Studies|  62|    5.4|Data Science Blac...|\n",
      "|      5|        Case Studies|  62|    5.5|    Data Science BMW|\n",
      "|      5|        Case Studies|  62|    5.6|Data Science Book...|\n",
      "|      5|        Case Studies|  62|    5.7|   Data Science CERN|\n",
      "|      5|        Case Studies|  62|    5.8| Data Science Disney|\n",
      "|      5|        Case Studies|  62|    5.9|    Data Science DLR|\n",
      "|      5|        Case Studies|  62|    5.1|Data Science Driv...|\n",
      "|      5|        Case Studies|  62|   5.11|Data Science Dropbox|\n",
      "|      5|        Case Studies|  62|   5.12|   Data Science Ebay|\n",
      "|      5|        Case Studies|  62|   5.13|Data Science Expedia|\n",
      "|      5|        Case Studies|  62|   5.14|Data Science Face...|\n",
      "|      5|        Case Studies|  62|   5.15| Data Science Google|\n",
      "+-------+--------------------+----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selet something specific\n",
    "bookcontentDF.where(bookcontentDF.Section_Name.contains(\"Data\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By + Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Chapter|count|\n",
      "+-------+-----+\n",
      "|      1|    4|\n",
      "|      6|    3|\n",
      "|      3|    8|\n",
      "|      5|   37|\n",
      "|      9|    3|\n",
      "|      4|    7|\n",
      "|      8|    1|\n",
      "|      7|   19|\n",
      "|      2|   11|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by aggregation options:\n",
    "# agg(additional functions), count, mean, max or min, pivot, sum\n",
    "bookcontentDF.groupBy(bookcontentDF.Chapter).count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
